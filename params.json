{"name":"S3fs-recipe","tagline":"s3fs chef recipe","body":"# s3fs cookbook\r\n\r\nInstall the [S3FS driver](https://code.google.com/p/s3fs/) for one or more of your Amazon S3 buckets. The cookbook supports using an encrypted data bag to keep data safe in shared situations.\r\n\r\nThe latest community release can be found at http://community.opscode.com/cookbooks/s3fs.\r\n\r\n# What does it do?\r\n\r\nIt will install S3FS on your server, then it will create folders in the `/mnt` directory named the same as each bucket listed in the data bag.  Lastly, it will create an S3FS mount for each S3 bucket specified in your configuration.\r\n\r\nThe recipe will handle encrypted data bags, and uses the `Chef::EncryptedDataBagItem.load` with default decryption key file.\r\n\r\n# Requirements\r\n\r\n## Platform:\r\n\r\n* CentOS\r\n* RHEL\r\n* Ubuntu\r\n* Debian\r\n\r\n# Usage\r\n\r\nTo use, create a data bag per each unique S3FS configuration. Here's an example:\r\n\r\n```json\r\n{\r\n  \"id\": \"deploy_key\",\r\n  \"buckets\": [ \"bucket1\", \"bucket2\" ],\r\n  \"access_key_id\": \"ABCDEFGHIJKLMNOPQRST\",\r\n  \"secret_access_key\": \"abcdefghijklmnopqrstuvwxyz01234567890ABC\"\r\n}\r\n```\r\n\r\nThen, for each node to run this configuration, use a role like this:\r\n\r\n```ruby\r\n\"run_list\": [\r\n  \"recipe[s3fs]\",\r\n  ...\r\n],\r\n\"override_attributes\": [\r\n  \"s3fs\": {\r\n    \"data_bag\": {\r\n      \"name\": \"s3_keys\",\r\n      \"item\": \"deploy_key\"\r\n    }\r\n  },\r\n  ...\r\n}\r\n```\r\n\r\nIt is encouraged, however, to build a wrapper cookbook to specify the necessary attributes, rather than using roles, following the Berkshelf model.\r\n\r\n## Multi User Support\r\n\r\nIf you have multiple AWS IAM users with different keys and multiple buckets that need mounted, you can use `node['s3fs']['multi_user']`. By setting this attribute to true, it will allow you to mount multiple buckets across as many user accounts (with different keys) as you want.\r\n\r\nInstead of using `node[\"s3fs\"][\"data_bag\"][\"item\"]` to specify which data bag item to load, you create one or more data bags under `node[\"s3fs\"][\"data_bag\"][\"name\"]` for each unique AWS IAM user you need to use. The data bag format is otherwise the same.\r\n\r\nAnd example of two separate data bag items in the same data bag:\r\n\r\n```json\r\n{\r\n  \"id\": \"s3fs_iam_1\",\r\n  \"buckets\": [ \"bucket1\", \"bucket2\" ],\r\n  \"access_key_id\": \"ABCDEFGHIJKLMNOPQRST\",\r\n  \"secret_access_key\": \"abcdefghijklmnopqrstuvwxyz01234567890ABC\"\r\n}\r\n\r\n{\r\n  \"id\": \"s3fs_iam_2\",\r\n  \"buckets\": [ \"bucket1\", \"bucket2\" ],\r\n  \"access_key_id\": \"ABCDEFGHIJKLMNOPQRST\",\r\n  \"secret_access_key\": \"abcdefghijklmnopqrstuvwxyz01234567890ABC\"\r\n}\r\n```\r\n\r\n# Attributes\r\n\r\nSee `attributes/default.rb` for defaults generated per platform.\r\n\r\n* `node[\"s3fs\"][\"packages\"]` - Set of packages needed to install S3FS\r\n* `node[\"fuse\"][\"version\"]` - The version of FUSE to install\r\n* `node[\"s3fs\"][\"version\"]` - The version of S3FS to install\r\n* `node[\"s3fs\"][\"mount_root\"]` - The root path for any mounted S3 buckets\r\n* `node[\"s3fs\"][\"multi_user\"]` - Enable multi-user support\r\n* `node[\"s3fs\"][\"options\"]` - Options to set when mounting a bucket to the filesystem\r\n* `node[\"s3fs\"][\"data_bag\"][\"name\"]` - The name of the data bag that contains an item with the buckets to mount & necessary AWS credentials\r\n* `node[\"s3fs\"][\"data_bag\"][\"item\"]` - The name of the data bag item that contains the buckets to mount & necessary AWS credentials\r\n\r\n# Recipes\r\n\r\n## default\r\n\r\nThe default recipe installs a set of packages necessary to build S3FS from source, installs FUSE, then builds and installs S3FS.\r\n\r\nIt then configures and mounts one of more S3 buckets, specified in a data bag's item or items, with the associated AWS credentials.\r\n\r\n# Author\r\n\r\nAuthor:: Tom Wilson <tom@jackhq.com>\r\n\r\nCopyright:: 2011 Tom Wilson\r\n\r\nsee LICENSE\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}